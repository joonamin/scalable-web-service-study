# Chapter2)<br> 대규모 데이터 처리 입문

`🗓️ 2024.06.08` ` 👩🏻 황진주 `
> 대규모 데이터 처리 환경에서 일어나는 주요 상황에 대해 정리합니다.

### 목차

- [Chapter2) 대규모 데이터 처리 입문](#chapter2-대규모-데이터-처리-입문)
    - [목차](#목차)
    - [하테나 북마크 데이터 규모](#하테나-북마크-데이터-규모)
  - [대규모 처리가 어려운 이유](#대규모-처리가-어려운-이유)
    - [1️⃣ 메모리 내부 계산이 어렵다](#1️⃣-메모리-내부-계산이-어렵다)
      - [사고 전개](#사고-전개)
    - [2️⃣ 메모리와 디스크의 속도차](#2️⃣-메모리와-디스크의-속도차)
      - [하드웨어 지연 발생점](#하드웨어-지연-발생점)
    - [3️⃣ DB 확장성 확보의 어려움](#3️⃣-db-확장성-확보의-어려움)
  - [🔆 부하 계측법 및 처리 방안 🔆](#-부하-계측법-및-처리-방안-)
    - [🧩 상황별 명령어](#-상황별-명령어)
    - [🙋‍♀️선생님 Load Average가 뭐죠?! 🤷‍♀️🤸‍♀️ 🤦‍♀️](#️선생님-load-average가-뭐죠-️️-️)
    - [🪓 병목 상황 별 대처 및 대응 순서](#-병목-상황-별-대처-및-대응-순서)
      - [✅ OS 튜닝](#-os-튜닝)
  - [🌠 sar](#-sar)
      - [CPU 사용률 조회](#cpu-사용률-조회)
      - [Load Average 사용률 조회](#load-average-사용률-조회)
      - [🙄 주의하세용](#-주의하세용)
      - [추가 옵션](#추가-옵션)
      - [참고 자료](#참고-자료)
  - [대규모 데이터를 다룰 때,,,](#대규모-데이터를-다룰-때)
      - [고민 시사점](#고민-시사점)

<br>

---

<br>

### 하테나 북마크 데이터 규모
 >
 > 하테나는 초대규모 규모를 지니지 않았지만 대/중규모의 사이즈를 지니고 있다.

- 레코드 : 1,500 만 ~ 5,000 만
- 단일 테이블
  - entry : 3GB
  - bookmark : 5.5GB
  - tag : 4.8 GB
  - relword : 10 GB
- 압축 HTML : 200 GB

 => 인덱스를 태우지 않는다면 굉장한 시간 소요

<br>

---

<br>

## 대규모 처리가 어려운 이유

### 1️⃣ 메모리 내부 계산이 어렵다

#### 사고 전개

 1) 대규모의 데이터를 모두 메모리에 올릴 수 없다.
 2) 자연스럽게 디스크 I/O가 발생
 3) 큰 시간 지연 발생

<br>

### 2️⃣ 메모리와 디스크의 속도차
 >
 > 메모리는 디스크보다 $10^5$ ~ $10^6$ 배 빠르다.

#### 하드웨어 지연 발생점

 1) 물리적 구조; 헤드의 이동 및 원반의 회전
 2) 단방향 회전구조
 3) 물리적 공간배치가 먼 데이터의 존재
 4) 전송 속도에서의 차이; 메모리 7.5GB/s, 디스크: 58MB/s

 \* 약 $10^{-3}$의 지연 발생

```
Linux의 hdparm을 통해 메모리와 디스크의 전송 속도차를 비교해 볼 수 있다.

1의 대안으로 SSD가 있지만 이 또한 버스 속도 혹은
그 외적인 구조로 인하여 메모리의 속도를 따라잡지는 못한다.

또한 2, 3과 같은 문제에 대해서는
OS 측에서 연관 데이터 대해서는 인접위치에 배치하거s나
회전 최소화 알고리즘을 이용해 극복하고자 하고 있다.
```

<br>

### 3️⃣ DB 확장성 확보의 어려움
> CPU 연산 속도의 문제는 대수를 늘리는 것으로 간단히 해결된다. <br> 하지만 DB I/O에 개선에 대해서는 그렇지 않다.

 - **CPU 부하의 규모조정** : 물리적 서버 증설 및 로드밸런서를 통한 분산
 - **I/O 부하의 규모조정** : 데이터 동기화 문제 해결 必

<br>

---

<br>

## 🔆 부하 계측법 및 처리 방안 🔆

### 🧩 상황별 명령어

| 상황 | 명령 |
|:---:|:---:|
| Load Average| top, uptime |
| 병목| top, sar |
| 프로세스 상태| ps |
| 추적, 프로파일링| strace, oprofile |
| I/O부하, swap 확인| sar, cmstat |
| CPU 사용, I/O 대기| sar |

### 🙋‍♀️선생님 Load Average가 뭐죠?! 🤷‍♀️🤸‍♀️ 🤦‍♀️
```
허허허, 단위 시간 당 대기 태스크 수를 알려주는게 Load Average입니다.
해당 태스크가 어느정도 대기상태로 있었는지 알면 부하가 높은 상황이라고 알 수 있죠.
그렇기에 "처리를 하고싶어도 실행을 못해서 대기하는 프로세스를 의미합니다."

- CPU 실행권한 대기 프로세스
- 디스크 I/O를 대기중인 프로세스

그래서 둘 중 어떤 건지 어떻게 아는진 바로 아래서 얘기하죠
```

<br><br>

### 🪓 병목 상황 별 대처 및 대응 순서

1.**Load Abverage 확인**
 > top, uptime을 이용한 시스템 전체 부하 확인

2.**CPU, I/O 중 병목 원인 조사**

 > sar/vmstat를 이용한 CPU 사용률 및 I/O  대기율 확인

3.**CPU 부하 발생**
> 프로그램 폭주로 인한 상태는 오류 제거, 전송량 문제에 대해서는 서버 증설 및 로직/알고리즘 개선으로 대응
  - 3.1 `top`/`sar`을 이용한 사용자/시스템 프로그램 중 원인 파악 
  - 3.2 'ps'를 이용한 프로세스 상태 및 CPU 사용 시간 확인
  - 3.3 'strace'를 이용한 추적 혹은 'oprofile'을 이용한 프로파일링

4. **I/O부하 발생**
> sar, vmstat를 통한 입출력 다량 발생 혹은 스왑으로 인한 상황 여부 파악
  - 4.1 ps를 이용한 극단적 메모리 이용 현황 파악
  - 4.2 메모리 증설 불가 시, 데이터 분산 및 캐시서버 검토

<br>

#### ✅ OS 튜닝

```
OS 성능 자체 향상을 위한 튜닝에 대한 내용이다.
소프트웨어의 개선점이 존재해야만 가능하며, 본래 하드웨어의 기대치를 넘을 수는 없다는 것을 명심.
아래 3가지를 확인해보자

- 메모리 증설로 대응이 되는가?
- 본래 데이터가 너무 많은것이 아닌가?
- I/O 알고리즘 변경의 필요가 존재하는가?
```

<br>

---

<br>

## 🌠 sar
> System Activity Report의 약자로 CPU, 메모리, 입출력 사용량 정보 수집 및 리포팅 담당

<br>

#### CPU 사용률 조회
```shell
$ sar -u 2 5
```
**결과**
```
Linux 2.6.32-754.11.1.el6.x86_64 (myhost)   06/01/2022      _x86_64_        (40 CPU)

~~~~05:15:53 PM     CPU     %user     %nice   %system   %iowait    %steal     %idle
05:15:55 PM     all     14.44      0.00      1.59      0.00      0.00     83.97
05:15:57 PM     all     15.71      0.00      1.70      0.00      0.00     82.59
05:15:59 PM     all     15.28      0.00      1.54      0.00      0.00     83.18
05:16:01 PM     all     17.74      0.00      4.71      0.00      0.00     77.55
05:16:03 PM     all     12.44      0.00      1.47      0.00      0.00     86.09
Average:        all     15.12      0.00      2.20      0.00      0.00     82.68
```
|결과|의미|
|:--:|:--|
|**%user**|사용자 모드에서 CPU가 사용된 시간의 비율|
|**%nice**|nice로 스케줄링의 우선순위를 변경한 프로세스가 사용자 모드에서 CPU를 사용한 시간 비율|
|**%system**|시스템 모드에서 CPU가 사용된 시간의 비율|
|**%iowait**|CPU가 디스크 입출력 대기를 위해 기다린 시간 비율|
|**%steal**|OS 가상화를 이용하고 있을 경우, 다른 가상 CPU의 계산으로 대기된 시간 비율|
|**%idle**|CPU가 사용되지 않고 유휴상태로 소비한 시간비율|

<br>

#### Load Average 사용률 조회
```shell
$ sar -q 2 5
```
**결과**
```
Linux 2.6.32-754.11.1.el6.x86_64 (myhost)   06/01/2022      _x86_64_        (40 CPU)

18시 25분 16초   runq-sz  plist-sz   ldavg-1   ldavg-5  ldavg-15   blocked
18시 25분 18초         1      3043      1.26      1.31      1.32         0
18시 25분 20초         1      2984      1.26      1.31      1.32         0
18시 25분 22초         2      2984      1.32      1.32      1.32         0
18시 25분 24초         1      2985      1.32      1.32      1.32         0
18시 25분 26초         1      2983      1.32      1.32      1.32         0
평균값:                1      2996      1.30      1.32      1.32         0
```
|결과|의미|
|:--:|:--|
|**runq-sz**|실행큐에 쌓여있는 프로세스 수|
|**plist-sz**|시스템상의 프로세스 사이즈|
|**ldavg-\***|1분, 5분, 15 사이의 Load Average 값|
|**blocked**|현재 처리되고 있는 입출력 작업의 숫자|

<br>

#### 🙄 주의하세용
> ❗❗ 멀티 CPU가 탑재되어 있더라도 디스크가 하나인 경우에는 CPU 부하가 다른 CPU로 분산되어도 I/O 부하는 분산 안됨 ❗📵❗

```shell
# -P 옵션을 통해 멀티코어 CPU에 대한 각 지표를 표출해보자
$ sar -P ALL | head
```

<br>

#### 추가 옵션
 - r : 메모리 사용량 정보
 - W : 스왑 메모리 정보 확인
 - d : 블록 디바이스 정보 수집
 - n : 네트워크 사용량 정보 출력
 - o : 파일로 저장

<br>

#### 참고 자료 
 - [옵션 출처: A6K 개발노트](https://hbase.tistory.com/326)
 - [좋참(좋은 참고자료라는 뜻) : "sar를 이용한 이슈 분석 사례" - 오픈소스컨설팅](https://tech.osci.kr/linux-sar%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9D%B4%EC%8A%88-%EB%B6%84%EC%84%9D-%EC%82%AC%EB%A1%80/)
 - [좋참 : ORACLE 시스템 관리 설명서 ](https://docs.oracle.com/cd/E24846_01/html/E23088/spmonitor-8.html#scrolltoc)
 - [좋참 : AWS EC2 인스턴스 모니터링 제안](https://repost.aws/ko/knowledge-center/ec2-linux-monitor-performance-with-sar)

<br>

---

<br>


## 대규모 데이터를 다룰 때,,,
> 를 다짐한 순간 부터 막막하다 어떤 걸 고려해야할까?


#### 고민 시사점
 - 🧏‍♀️ 어떻게 메모리에서 처리를 마치치?
   - 디스크 seek 횟수 최소화 및 국소성을 활용한 분산
 - 🚥 현재 데이터 량에는 어떤 알고리즘과 구조가 맞지?
   - 이분 탐색을 한다던가... 트리 구조로 변환해본단던가...
 -  🚅 데이터 압축과 검색은 어떻게 할까?
    -  작은 데이터는 검색 및 캐싱에 유리함